{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f365191d-69fd-4371-be0d-ab02135eba1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data-Intensive Programming - Assignment\n",
    "\n",
    "This is the **Python** version of the assignment. Switch to the Scala version, if you want to do the assignment in Scala.\n",
    "\n",
    "In all tasks, add your solutions to the cells following the task instructions. You are free to add new cells if you want.\n",
    "\n",
    "Don't forget to **submit your solutions to Moodle** once your group is finished with the assignment.\n",
    "\n",
    "## Basic tasks (compulsory)\n",
    "\n",
    "There are in total seven basic tasks that every group must implement in order to have an accepted assignment.\n",
    "\n",
    "The basic task 1 is a warming up task and it deals with some video game sales data. The task asks you to do some basic aggregation operations with Spark data frames.\n",
    "\n",
    "The other basic tasks (basic tasks 2-7) are all related and deal with data from [https://moneypuck.com/data.htm](https://moneypuck.com/data.htm) that contains information about every shot in all National Hockey League ([NHL](https://en.wikipedia.org/wiki/National_Hockey_League), [ice hockey](https://en.wikipedia.org/wiki/Ice_hockey)) matches starting from season 2011-12 and ending with the last completed season, 2022-23. The tasks ask you to calculate the results of the matches based on the given data as well as do some further calculations. Knowledge about ice hockey or NHL is not required, and the task instructions should be sufficient in order to gain enough context for the tasks.\n",
    "\n",
    "## Additional tasks (optional, can provide course points)\n",
    "\n",
    "There are in total of three additional tasks that can be done to gain some course points.\n",
    "\n",
    "The first additional task asks you to do all the basic tasks in an optimized way. It is possible that you can some points from this without directly trying by just implementing the basic tasks in an efficient manner.\n",
    "\n",
    "The other two additional tasks are separate tasks and do not relate to any other basic or additional tasks. One of them asks you to load in unstructured text data and do some calculations based on the words found from the data. The other asks you to utilize the K-Means algorithm to partition the given building data.\n",
    "\n",
    "It is possible to gain partial points from the additional tasks. I.e., if you have not completed the task fully but have implemented some part of the task, you might gain some appropriate portion of the points from the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0620f9-57df-46eb-8040-766a6bf156a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import statements for the entire notebook\n",
    "\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as Func, DataFrame\n",
    "from pyspark.sql.functions import sum, col,round,dense_rank,count, expr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed64a30-122e-4363-8934-48ff43610bce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 1 - Sales data\n",
    "\n",
    "The CSV file `assignment/sales/video_game_sales.csv` in the [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) contains video game sales data (from [https://www.kaggle.com/datasets/ashaheedq/video-games-sales-2019/data](https://www.kaggle.com/datasets/ashaheedq/video-games-sales-2019/data)). The direct address for the dataset is: `abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/sales/video_game_sales.csv`\n",
    "\n",
    "Load the data from the CSV file into a data frame. The column headers and the first few data lines should give sufficient information about the source dataset.\n",
    "\n",
    "Only data for sales in the first ten years of the 21st century should be considered in this task, i.e. years 2000-2009.\n",
    "\n",
    "Using the data, find answers to the following:\n",
    "\n",
    "- Which publisher had the highest total sales in video games in European Union in years 2000-2009?\n",
    "- What were the total yearly sales, in European Union and globally, for this publisher in year 2000-2009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea55fbb5-ab28-4457-9d21-a265a8d73b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publisher with the highest total video game sales in European Union is: 'Nintendo'\nSales data for the publisher:\n+----+--------+------------+\n|Year|EU_Total|Global_Total|\n+----+--------+------------+\n|2000|    6.42|       34.05|\n|2001|    8.84|       45.37|\n|2002|    9.89|       48.31|\n|2003|    7.08|       38.14|\n|2004|   12.43|       60.65|\n|2005|   42.69|      127.47|\n|2006|   60.35|      205.61|\n|2007|   32.81|      104.18|\n|2008|   25.13|       91.22|\n|2009|   36.18|      128.89|\n+----+--------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "#file path\n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/sales/video_game_sales.csv\"\n",
    "\n",
    "#reading\n",
    "video_game = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Filtering\n",
    "video_game_Year = video_game.filter((video_game['Year'] >= 2000) & (video_game['Year'] <= 2009))\n",
    "\n",
    "\n",
    "# Calculate total sales in European Union for each publisher\n",
    "bestEUPublisherEu = video_game_Year.groupBy('Publisher').agg(\n",
    "    sum(col('EU_Sales').cast('double')).alias('EU_Total')\n",
    ")\n",
    "\n",
    "# Find the publisher with the highest total sales in European Union\n",
    "bestEUPublisher:str = bestEUPublisherEu.orderBy('EU_Total', ascending=False).first()['Publisher']\n",
    "\n",
    "#highest total sales\n",
    "bestEUPublisher_match = video_game_Year.filter(video_game_Year['Publisher'] == bestEUPublisher)\n",
    "\n",
    "\n",
    "# Best publisher sales by Year\n",
    "bestEUPublisherSales = bestEUPublisher_match.groupBy('Year').agg( round(sum('EU_Sales'),2).alias('EU_Total'),\n",
    "                                             round(sum('Global_Sales'),2).alias('Global_Total')).orderBy('Year')\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The publisher with the highest total video game sales in European Union is: '{bestEUPublisher}'\")\n",
    "print(\"Sales data for the publisher:\")\n",
    "bestEUPublisherSales.show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183677cf-0a1b-4596-996e-fe7dc5972dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 2 - Shot data from NHL matches\n",
    "\n",
    "A parquet file in the [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) at folder `assignment/nhl_shots.parquet` from [https://moneypuck.com/data.htm](https://moneypuck.com/data.htm) contains information about every shot in all National Hockey League ([NHL](https://en.wikipedia.org/wiki/National_Hockey_League), [ice hockey](https://en.wikipedia.org/wiki/Ice_hockey)) matches starting from season 2011-12 and ending with the last completed season, 2022-23.\n",
    "\n",
    "In this task you should load the data with all of the rows into a data frame. This data frame object will then be used in the following basic tasks 3-7.\n",
    "\n",
    "### Background information\n",
    "\n",
    "Each NHL season is divided into regular season and playoff season. In the regular season the teams play up to 82 games with the best teams continuing to the playoff season. During the playoff season the remaining teams are paired and each pair play best-of-seven series of games to determine which team will advance to the next phase.\n",
    "\n",
    "In ice hockey each game has a home team and an away team. The regular length of a game is three 20 minute periods, i.e. 60 minutes or 3600 seconds. The team that scores more goals in the regulation time is the winner of the game.\n",
    "\n",
    "If the scoreline is even after this regulation time:\n",
    "\n",
    "- In playoff games, the game will be continued until one of the teams score a goal with the scoring team being the winner.\n",
    "- In regular season games, there is an extra time that can last a maximum of 5 minutes (300 seconds). If one of the teams score, the game ends with the scoring team being the winner. If there is no goals in the extra time, there would be a shootout competition to determine the winner. These shootout competitions are not considered in this assignment, and the shots from those are not included in the raw data.\n",
    "\n",
    "**Columns in the data**\n",
    "\n",
    "Each row in the given data represents one shot in a game.\n",
    "\n",
    "The column description from the source website. Not all of these will be needed in this assignment.\n",
    "\n",
    "| column name | column type | description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| shotID      | integer | Unique id for each shot |\n",
    "| homeTeamCode | string | The home team in the game. For example: TOR, MTL, NYR, etc. |\n",
    "| awayTeamCode | string | The away team in the game |\n",
    "| season | integer | Season the shot took place in. Example: 2009 for the 2009-2010 season |\n",
    "| isPlayOffGame | integer | Set to 1 if a playoff game, otherwise 0 |\n",
    "| game_id | integer | The NHL Game_id of the game the shot took place in |\n",
    "| time | integer | Seconds into the game of the shot |\n",
    "| period | integer | Period of the game |\n",
    "| team | string | The team taking the shot. HOME or AWAY |\n",
    "| location | string | The zone the shot took place in. HOMEZONE, AWAYZONE, or Neu. Zone |\n",
    "| event | string | Whether the shot was a shot on goal (SHOT), goal, (GOAL), or missed the net (MISS) |\n",
    "| homeTeamGoals | integer | Home team goals before the shot took place |\n",
    "| awayTeamGoals | integer | Away team goals before the shot took place |\n",
    "| homeTeamWon | integer | Set to 1 if the home team won the game. Otherwise 0. |\n",
    "| shotType | string | Type of the shot. (Slap, Wrist, etc) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b47dfe-e2d7-4402-99df-2d23e8cbe2cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1279180\nNumber of columns: 15\n"
     ]
    }
   ],
   "source": [
    "# File path for the parquet file\n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/nhl_shots.parquet\"\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "shotsDF: DataFrame = spark.read.parquet(file_path)\n",
    "\n",
    "\n",
    "# Number of rows\n",
    "num_rows = shotsDF.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "# Number of columns\n",
    "num_columns = len(shotsDF.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fcc237-0014-4be8-943f-96c063c3a53c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 3 - Game data frame\n",
    "\n",
    "Create a match data frame for all the game included in the shots data frame created in basic task 2.\n",
    "\n",
    "The output should contain one row for each game.\n",
    "\n",
    "The following columns should be included in the final data frame for this task:\n",
    "\n",
    "| column name    | column type | description |\n",
    "| -------------- | ----------- | ----------- |\n",
    "| season         | integer     | Season the game took place in. Example: 2009 for the 2009-2010 season |\n",
    "| game_id        | integer     | The NHL Game_id of the game |\n",
    "| homeTeamCode   | string      | The home team in the game. For example: TOR, MTL, NYR, etc. |\n",
    "| awayTeamCode   | string      | The away team in the game |\n",
    "| isPlayOffGame  | integer     | Set to 1 if a playoff game, otherwise 0 |\n",
    "| homeTeamGoals  | integer     | Number of goals scored by the home team |\n",
    "| awayTeamGoals  | integer     | Number of goals scored by the away team |\n",
    "| lastGoalTime   | integer     | The time in seconds for the last goal in the game. 0 if there was no goals in the game. |\n",
    "\n",
    "All games had at least some shots but there are some games that did not have any goals either in the regulation 60 minutes or in the extra time.\n",
    "\n",
    "Note, that for a couple of games there might be some shots, including goal-scoring ones, that are missing from the original dataset. For example, there might be a game with a final scoreline of 3-4 but only 6 of the goal-scoring shots are included in the dataset. Your solution does not have to try to take these rare occasions of missing data into account. I.e., you can do all the tasks with the assumption that there are no missing or invalid data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1bb4abd-62e4-4a3d-a331-c0b41f8c82c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 15079\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|season|game_id|homeTeamCode|awayTeamCode|isPlayOffGame|homeTeamGoals|awayTeamGoals|lastGoalTime|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|  2021|  20358|         NSH|         BOS|            0|            0|            2|        3599|\n|  2019|  20108|         PIT|         DAL|            0|            4|            2|        3588|\n|  2011|  30183|         DET|         NSH|            1|            2|            3|        3546|\n|  2012|  20587|         WPG|         BUF|            0|            4|            1|        3574|\n|  2017|  20952|         N.J|         NYI|            0|            2|            1|        3555|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\nonly showing top 5 rows\n\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|season|game_id|homeTeamCode|awayTeamCode|isPlayOffGame|homeTeamGoals|awayTeamGoals|lastGoalTime|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|  2012|  20556|         COL|         DET|            0|            2|            3|        3884|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a temporary view for the shots DataFrame\n",
    "shotsDF.createOrReplaceTempView(\"shots\")\n",
    "\n",
    "# Define the games DataFrame using Spark SQL\n",
    "gamesDF = spark.sql(\"\"\"\n",
    "    SELECT season, \n",
    "           game_id, \n",
    "           homeTeamCode, \n",
    "           awayTeamCode, \n",
    "           isPlayOffGame,\n",
    "           SUM(CASE WHEN team = 'HOME' AND event = 'GOAL' THEN 1 ELSE 0 END) as homeTeamGoals,\n",
    "           SUM(CASE WHEN team = 'AWAY' AND event = 'GOAL' THEN 1 ELSE 0 END) as awayTeamGoals,\n",
    "           MAX(CASE WHEN homeTeamGoals > 0 OR awayTeamGoals > 0 THEN time ELSE 0 END) as lastGoalTime\n",
    "    FROM shots\n",
    "    GROUP BY season, game_id, homeTeamCode, awayTeamCode, isPlayOffGame\n",
    "\"\"\")\n",
    "\n",
    "# Cache the DataFrame\n",
    "gamesDF.cache()\n",
    "\n",
    "# Display the number of rows and show the first 5 rows\n",
    "print(\"Number of rows:\", gamesDF.count())\n",
    "gamesDF.show(5)\n",
    "\n",
    "#checking out the output for secific one\n",
    "filtered_game = gamesDF.filter(\n",
    "    (gamesDF['game_id'] == 20556 ) &\n",
    "    (gamesDF['homeTeamCode'] == 'COL') &\n",
    "    (gamesDF['awayTeamCode'] == 'DET')\n",
    ")\n",
    "# Display the filtered game\n",
    "filtered_game.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a49eb6-38e6-46b8-a6d9-12121840f6c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 4 - Game wins during playoff seasons\n",
    "\n",
    "Create a data frame that uses the game data frame from the basic task 3 and contains aggregated number of wins and losses for each team and for each playoff season, i.e. for games which have been marked as playoff games. Only teams that have played in at least one playoff game in the considered season should be included in the final data frame.\n",
    "\n",
    "The following columns should be included in the final data frame:\n",
    "\n",
    "| column name    | column type | description |\n",
    "| -------------- | ----------- | ----------- |\n",
    "| season         | integer     | The season for the data. Example: 2009 for the 2009-2010 season |\n",
    "| teamCode       | string      | The code for the team. For example: TOR, MTL, NYR, etc. |\n",
    "| games          | integer     | Number of playoff games the team played in the given season |\n",
    "| wins           | integer     | Number of wins in playoff games the team had in the given season |\n",
    "| losses         | integer     | Number of losses in playoff games the team had in the given season |\n",
    "\n",
    "Playoff games where a team scored more goals than their opponent are considered winning games. And playoff games where a team scored less goals than the opponent are considered losing games.\n",
    "\n",
    "In real life there should not be any playoff games where the final score line was even but due to some missing shot data you might end up with a couple of playoff games that seems to have ended up in a draw. These possible \"drawn\" playoff games can be left out from the win/loss calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3934cd2-8af7-4ac8-a3b3-8ae497bbabc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+\n|season|teamCode|games|wins|losses|\n+------+--------+-----+----+------+\n|  2012|     PIT|   15|   8|     7|\n|  2011|     NYR|   20|  10|    10|\n|  2020|     CAR|   11|   5|     6|\n|  2021|     CAR|   14|   7|     7|\n|  2012|     OTT|   10|   5|     5|\n|  2011|     DET|    5|   1|     4|\n|  2014|     CHI|   23|  16|     7|\n|  2020|     T.B|   23|  16|     7|\n|  2019|     FLA|    4|   1|     3|\n|  2012|     MIN|    5|   1|     4|\n+------+--------+-----+----+------+\nonly showing top 10 rows\n\n+------+--------+-----+----+------+\n|season|teamCode|games|wins|losses|\n+------+--------+-----+----+------+\n|  2021|     NYR|   20|  10|    10|\n+------+--------+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter playoff games\n",
    "playoffGamesDF = gamesDF.filter(gamesDF['isPlayOffGame'] == 1)\n",
    "\n",
    "# Calculate wins and losses for home teams\n",
    "homeWinsLossesDF = playoffGamesDF.groupBy(\"season\", \"homeTeamCode\").agg(\n",
    "    F.sum(F.when(playoffGamesDF['homeTeamGoals'] > playoffGamesDF['awayTeamGoals'], 1).otherwise(0)).alias(\"wins\"),\n",
    "    F.sum(F.when(playoffGamesDF['homeTeamGoals'] < playoffGamesDF['awayTeamGoals'], 1).otherwise(0)).alias(\"losses\"),\n",
    "    F.count(\"game_id\").alias(\"games\")\n",
    ").withColumnRenamed(\"homeTeamCode\", \"teamCode\")\n",
    "\n",
    "# Calculate wins and losses for away teams\n",
    "awayWinsLossesDF = playoffGamesDF.groupBy(\"season\", \"awayTeamCode\").agg(\n",
    "    F.sum(F.when(playoffGamesDF['awayTeamGoals'] > playoffGamesDF['homeTeamGoals'], 1).otherwise(0)).alias(\"wins\"),\n",
    "    F.sum(F.when(playoffGamesDF['awayTeamGoals'] < playoffGamesDF['homeTeamGoals'], 1).otherwise(0)).alias(\"losses\"),\n",
    "    F.count(\"game_id\").alias(\"games\")\n",
    ").withColumnRenamed(\"awayTeamCode\", \"teamCode\")\n",
    "\n",
    "# Combine home and away results\n",
    "combinedDF = homeWinsLossesDF.union(awayWinsLossesDF)\n",
    "\n",
    "# Aggregate total wins and losses for each team and season\n",
    "playoffDF = combinedDF.groupBy(\"season\", \"teamCode\").agg(\n",
    "    F.sum(\"games\").alias(\"games\"),\n",
    "    F.sum(\"wins\").alias(\"wins\"),\n",
    "    F.sum(\"losses\").alias(\"losses\"),\n",
    "    \n",
    ")\n",
    "# Filter out teams that did not play any playoff games\n",
    "playoffDF = playoffDF.filter(playoffDF['games'] > 0)\n",
    "\n",
    "# Display the results\n",
    "playoffDF.show(10)\n",
    "\n",
    "# Checking Filter for a specific team and season\n",
    "filtered_game = playoffDF.filter(\n",
    "    (playoffDF['season'] == 2021) &\n",
    "    (playoffDF['teamCode'] == 'NYR')\n",
    ")\n",
    "\n",
    "filtered_game.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab19f70c-d951-460d-a150-e53ea2ce0b20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 5 - Best playoff teams\n",
    "\n",
    "Using the playoff data frame created in basic task 4 create a data frame containing the win-loss record for best playoff team, i.e. the team with the most wins, for each season. You can assume that there are no ties for the highest amount of wins in each season.\n",
    "\n",
    "The following columns should be included in the final data frame:\n",
    "\n",
    "| column name    | column type | description |\n",
    "| -------------- | ----------- | ----------- |\n",
    "| season         | integer     | The season for the data. Example: 2009 for the 2009-2010 season |\n",
    "| teamCode       | string      | The team code for the best performing playoff team in the given season. For example: TOR, MTL, NYR, etc. |\n",
    "| games          | integer     | Number of playoff games the best performing playoff team played in the given season |\n",
    "| wins           | integer     | Number of wins in playoff games the best performing playoff team had in the given season |\n",
    "| losses         | integer     | Number of losses in playoff games the best performing playoff team had in the given season |\n",
    "\n",
    "Finally, fetch the details for the best playoff team in season 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb2bf37-ebbb-4657-86f2-87f2af64e517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+\n|season|teamCode|games|wins|losses|\n+------+--------+-----+----+------+\n|  2011|     L.A|   20|  16|     4|\n|  2012|     CHI|   23|  15|     7|\n|  2013|     L.A|   26|  16|    10|\n|  2014|     CHI|   23|  16|     7|\n|  2015|     PIT|   24|  16|     8|\n|  2016|     PIT|   25|  16|     9|\n|  2017|     WSH|   24|  16|     8|\n|  2018|     STL|   26|  16|    10|\n|  2019|     T.B|   25|  18|     7|\n|  2020|     T.B|   23|  16|     7|\n|  2021|     COL|   20|  16|     4|\n|  2022|     VGK|   22|  16|     6|\n+------+--------+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "windowWins = Window.partitionBy(\"season\").orderBy(F.desc(\"wins\"))\n",
    "\n",
    "rankedTeams = playoffDF.withColumn(\"rank\", F.row_number().over(windowWins))\n",
    "\n",
    "bestPlayoffTeams: DataFrame = rankedTeams.filter( \"rank==1\").select(\n",
    "            \"season\",\"teamCode\", \"games\",\"wins\",\"losses\")\n",
    "\n",
    "bestPlayoffTeams.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08fa662-5239-4fdd-ab45-135844e06f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best playoff team in 2022:\n    Team: VGK\n    Games: 22\n    Wins: 16\n    Losses: 6\n=========================================================\n"
     ]
    }
   ],
   "source": [
    "bestPlayoffTeam2022: Row = bestPlayoffTeams.filter(\"season == 2022\").first()\n",
    "\n",
    "\n",
    "bestPlayoffTeam2022Dict: dict = bestPlayoffTeam2022.asDict()\n",
    "print(\"Best playoff team in 2022:\")\n",
    "print(f\"    Team: {bestPlayoffTeam2022Dict.get('teamCode')}\")\n",
    "print(f\"    Games: {bestPlayoffTeam2022Dict.get('games')}\")\n",
    "print(f\"    Wins: {bestPlayoffTeam2022Dict.get('wins')}\")\n",
    "print(f\"    Losses: {bestPlayoffTeam2022Dict.get('losses')}\")\n",
    "print(\"=========================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f7895a-8533-4663-9e84-e95fd5fad079",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 6 - Regular season points\n",
    "\n",
    "Create a data frame that uses the game data frame from the basic task 3 and contains aggregated data for each team and for each season for the regular season matches, i.e. the non-playoff matches.\n",
    "\n",
    "The following columns should be included in the final data frame:\n",
    "\n",
    "| column name    | column type | description |\n",
    "| -------------- | ----------- | ----------- |\n",
    "| season         | integer     | The season for the data. Example: 2009 for the 2009-2010 season |\n",
    "| teamCode       | string      | The code for the team. For example: TOR, MTL, NYR, etc. |\n",
    "| games          | integer     | Number of non-playoff games the team played in the given season |\n",
    "| wins           | integer     | Number of wins in non-playoff games the team had in the given season |\n",
    "| losses         | integer     | Number of losses in non-playoff games the team had in the given season |\n",
    "| goalsScored    | integer     | Total number goals scored by the team in non-playoff games in the given season |\n",
    "| goalsConceded  | integer     | Total number goals scored against the team in non-playoff games in the given season |\n",
    "| points         | integer     | Total number of points gathered by the team in non-playoff games in the given season |\n",
    "\n",
    "Points from each match are received as follows (in the context of this assignment, these do not exactly match the NHL rules):\n",
    "\n",
    "| points | situation |\n",
    "| ------ | --------- |\n",
    "| 3      | team scored more goals than the opponent during the regular 60 minutes |\n",
    "| 2      | the score line was even after 60 minutes but the team scored a winning goal during the extra time |\n",
    "| 1      | the score line was even after 60 minutes but the opponent scored a winning goal during the extra time or there were no goals in the extra time |\n",
    "| 0      | the opponent scored more goals than the team during the regular 60 minutes |\n",
    "\n",
    "In the regular season the following table shows how wins and losses should be considered (in the context of this assignment):\n",
    "\n",
    "| win | loss | situation |\n",
    "| --- | ---- | --------- |\n",
    "| Yes | No   | team gained at least 2 points from the match |\n",
    "| No  | Yes  | team gain at most 1 point from the match |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db38d6be-a16c-4069-bda6-3081ff73347d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2021|     CBJ|   82|  33|    49|        258|          297|   103|\n|  2012|     PIT|   48|  33|    15|        162|          119|   100|\n|  2018|     ANA|   82|  31|    51|        195|          249|   102|\n|  2011|     NYR|   82|  47|    35|        222|          181|   144|\n|  2020|     CAR|   56|  31|    25|        175|          133|   103|\n+------+--------+-----+----+------+-----------+-------------+------+\nonly showing top 5 rows\n\n+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2022|     CGY|   82|  36|    46|        258|          247|   122|\n+------+--------+-----+----+------+-----------+-------------+------+\n\nNumber of rows: 368\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, count, sum\n",
    "\n",
    "home_regularSeasonDF = gamesDF.filter(col(\"isPlayOffGame\") == 0) \\\n",
    "    .withColumn(\"points\",\n",
    "                when(col(\"lastGoalTime\") == 0, 1).\n",
    "                when((col(\"lastGoalTime\") <= 3600) & (col(\"homeTeamGoals\") < col(\"awayTeamGoals\")), 0).\n",
    "                when((col(\"lastGoalTime\") <= 3600) & (col(\"homeTeamGoals\") > col(\"awayTeamGoals\")), 3).\n",
    "                when((col(\"lastGoalTime\") <= 3600) & (col(\"homeTeamGoals\") == col(\"awayTeamGoals\")), 1).\n",
    "                when((col(\"lastGoalTime\") > 3600) & (col(\"homeTeamGoals\") <= col(\"awayTeamGoals\")), 1).\n",
    "                when((col(\"lastGoalTime\") > 3600) & (col(\"homeTeamGoals\") > col(\"awayTeamGoals\")), 2)) \\\n",
    "    .withColumn(\"loss\", when(col(\"points\") < 2, 1).otherwise(0)) \\\n",
    "    .withColumn(\"win\", when(col(\"points\") >= 2, 1).otherwise(0)) \\\n",
    "    .groupBy(\"season\", \"homeTeamCode\") \\\n",
    "    .agg(count(\"game_id\").alias(\"games\"),\n",
    "         sum(\"win\").alias(\"wins\"),\n",
    "         sum(\"loss\").alias(\"losses\"),\n",
    "         sum(\"points\").alias(\"points\"),\n",
    "         sum(\"homeTeamGoals\").alias(\"goalsScored\"),\n",
    "         sum(\"awayTeamGoals\").alias(\"goalsConceded\")) \\\n",
    "    .withColumnRenamed(\"homeTeamCode\", \"teamCode\")\n",
    "\n",
    "away_regularSeasonDF = gamesDF.filter(col(\"isPlayOffGame\") == 0) \\\n",
    "    .withColumn(\"points\",\n",
    "                when(col(\"lastGoalTime\") == 0, 1).\n",
    "                when((col(\"lastGoalTime\") <= 3600) & (col(\"awayTeamGoals\") < col(\"homeTeamGoals\")), 0).\n",
    "                when((col(\"lastGoalTime\") <= 3600) & (col(\"awayTeamGoals\") > col(\"homeTeamGoals\")), 3).\n",
    "                when((col(\"lastGoalTime\") <= 3600) & (col(\"homeTeamGoals\") == col(\"awayTeamGoals\")), 1).\n",
    "                when((col(\"lastGoalTime\") > 3600) & (col(\"awayTeamGoals\") <= col(\"homeTeamGoals\")), 1).\n",
    "                when((col(\"lastGoalTime\") > 3600) & (col(\"awayTeamGoals\") > col(\"homeTeamGoals\")), 2)) \\\n",
    "    .withColumn(\"loss\", when(col(\"points\") < 2, 1).otherwise(0)) \\\n",
    "    .withColumn(\"win\", when(col(\"points\") >= 2, 1).otherwise(0)) \\\n",
    "    .groupBy(\"season\", \"awayTeamCode\") \\\n",
    "    .agg(count(\"game_id\").alias(\"games\"),\n",
    "         sum(\"win\").alias(\"wins\"),\n",
    "         sum(\"loss\").alias(\"losses\"),\n",
    "         sum(\"points\").alias(\"points\"),\n",
    "         sum(\"awayTeamGoals\").alias(\"goalsScored\"),\n",
    "         sum(\"homeTeamGoals\").alias(\"goalsConceded\")) \\\n",
    "    .withColumnRenamed(\"awayTeamCode\", \"teamCode\")\n",
    "\n",
    "regularSeasonDF = home_regularSeasonDF.union(away_regularSeasonDF) \\\n",
    "    .groupBy(\"season\", \"teamCode\") \\\n",
    "    .agg(sum(\"games\").alias(\"games\"),\n",
    "         sum(\"wins\").alias(\"wins\"),\n",
    "         sum(\"losses\").alias(\"losses\"),\n",
    "         sum(\"goalsScored\").alias(\"goalsScored\"),\n",
    "         sum(\"goalsConceded\").alias(\"goalsConceded\"),\n",
    "         sum(\"points\").alias(\"points\"),)\n",
    "\n",
    "regularSeasonDF.show(5)\n",
    "\n",
    "#for checking output for specific one\n",
    "outputDF = regularSeasonDF.filter(\n",
    "    (regularSeasonDF['season'] == 2022) &\n",
    "    (regularSeasonDF['teamCode'] == 'CGY') )\n",
    "\n",
    "outputDF.show()\n",
    "num_rows = regularSeasonDF.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef825eca-9bec-4255-ae99-33a8ab5467a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Task 7 - The worst regular season teams\n",
    "\n",
    "Using the regular season data frame created in the basic task 6, create a data frame containing the regular season records for the worst regular season team, i.e. the team with the least amount of points, for each season. You can assume that there are no ties for the lowest amount of points in each season.\n",
    "\n",
    "Finally, fetch the details for the worst regular season team in season 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9222c665-36f5-4d49-a51d-81294f99640a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2011|     CBJ|   82|  25|    57|        197|          257|    84|\n|  2012|     FLA|   48|  12|    36|        107|          170|    44|\n|  2013|     BUF|   82|  14|    68|        149|          242|    56|\n|  2014|     BUF|   82|  15|    67|        153|          268|    60|\n|  2015|     TOR|   82|  23|    59|        192|          240|    83|\n|  2016|     COL|   82|  21|    61|        165|          276|    61|\n|  2017|     BUF|   82|  24|    58|        198|          277|    80|\n|  2018|     OTT|   82|  29|    53|        243|          299|    88|\n|  2019|     DET|   71|  14|    57|        142|          265|    49|\n|  2020|     BUF|   56|  11|    45|        133|          196|    44|\n|  2021|     MTL|   82|  19|    63|        216|          316|    68|\n|  2022|     ANA|   82|  20|    62|        205|          335|    68|\n+------+--------+-----+----+------+-----------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "windowDeptDF = Window.partitionBy(\"season\").orderBy(col(\"points\"))\n",
    "worstRegularTeams = regularSeasonDF.withColumn(\"rank\", dense_rank().over(windowDeptDF)) \\\n",
    "    .where(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "worstRegularTeams.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505b463f-1b33-4ee8-b90f-7fc87b530126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst regular season team in 2022:\n    Team: ANA\n    Games: 82\n    Wins: 20\n    Losses: 62\n    Goals scored: 205\n    Goals conceded: 335\n    Points: 68\n"
     ]
    }
   ],
   "source": [
    "worstRegularTeam2022: Row = worstRegularTeams.filter(col(\"season\") == 2022).take(1)[0]\n",
    "\n",
    "worstRegularTeam2022Dict: dict = worstRegularTeam2022.asDict()\n",
    "print(\"Worst regular season team in 2022:\")\n",
    "print(f\"    Team: {worstRegularTeam2022Dict.get('teamCode')}\")\n",
    "print(f\"    Games: {worstRegularTeam2022Dict.get('games')}\")\n",
    "print(f\"    Wins: {worstRegularTeam2022Dict.get('wins')}\")\n",
    "print(f\"    Losses: {worstRegularTeam2022Dict.get('losses')}\")\n",
    "print(f\"    Goals scored: {worstRegularTeam2022Dict.get('goalsScored')}\")\n",
    "print(f\"    Goals conceded: {worstRegularTeam2022Dict.get('goalsConceded')}\")\n",
    "print(f\"    Points: {worstRegularTeam2022Dict.get('points')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d098fb-9998-4ace-88e9-2018b55edf17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional tasks\n",
    "\n",
    "The implementation of the basic tasks is compulsory for every group.\n",
    "\n",
    "Doing the following additional tasks you can gain course points which can help in getting a better grade from the course (or passing the course).\n",
    "Partial solutions can give partial points.\n",
    "\n",
    "The additional task 1 will be considered in the grading for every group based on their solutions for the basic tasks.\n",
    "\n",
    "The additional tasks 2 and 3 are separate tasks that do not relate to any other task in the assignment. The solutions used in these other additional tasks do not affect the grading of additional task 1. Instead, a good use of optimized methods can positively affect the grading of each specific task, while very non-optimized solutions can have a negative effect on the task grade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e05c1fd-2795-46c3-b65d-53569886c552",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional Task 1 - Optimized solutions to the basic tasks (2 points)\n",
    "\n",
    "Use the tools Spark offers effectively and avoid unnecessary operations in the code for the basic tasks.\n",
    "\n",
    "A couple of things to consider (**NOT** even close to a complete list):\n",
    "\n",
    "- Consider using explicit schemas when dealing with CSV data sources.\n",
    "- Consider only including those columns from a data source that are actually needed.\n",
    "- Filter unnecessary rows whenever possible to get smaller datasets.\n",
    "- Avoid collect or similar expensive operations for large datasets.\n",
    "- Consider using explicit caching if some data frame is used repeatedly.\n",
    "- Avoid unnecessary shuffling (for example sorting) operations.\n",
    "\n",
    "It is okay to have your own test code that would fall into category of \"ineffective usage\" or \"unnecessary operations\" while doing the assignment tasks. However, for the final Moodle submission you should comment out or delete such code (and test that you have not broken anything when doing the final modifications).\n",
    "\n",
    "Note, that you should not do the basic tasks again for this additional task, but instead modify your basic task code with more efficient versions.\n",
    "\n",
    "You can create a text cell below this one and describe what optimizations you have done. This might help the grader to better recognize how skilled your work with the basic tasks has been.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3913a94f-7e97-47d1-a615-2391b2a82662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publisher with the highest total video game sales in European Union is: 'Nintendo'\nSales data for the publisher:\n+----+--------+------------+\n|Year|EU_Total|Global_Total|\n+----+--------+------------+\n|2000|    6.42|       34.05|\n|2001|    8.84|       45.37|\n|2002|    9.89|       48.31|\n|2003|    7.08|       38.14|\n|2004|   12.43|       60.65|\n|2005|   42.69|      127.47|\n|2006|   60.35|      205.61|\n|2007|   32.81|      104.18|\n|2008|   25.13|       91.22|\n|2009|   36.18|      128.89|\n+----+--------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Basic task 1\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Platform\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Genre\", StringType(), True),\n",
    "    StructField(\"Publisher\", StringType(), True),\n",
    "    StructField(\"NA_Sales\", DoubleType(), True),\n",
    "    StructField(\"EU_Sales\", DoubleType(), True),\n",
    "    StructField(\"JP_Sales\", DoubleType(), True),\n",
    "    StructField(\"Other_Sales\", DoubleType(), True),\n",
    "    StructField(\"Global_Sales\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# File path\n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/sales/video_game_sales.csv\"\n",
    "\n",
    "# Reading with specific schema\n",
    "video_game = spark.read.schema(schema).csv(file_path, header=True)\n",
    "\n",
    "# Filtering for the specified year range\n",
    "video_game_Year = video_game.filter((video_game['Year'] >= 2000) & (video_game['Year'] <= 2009))\n",
    "\n",
    "# Calculate total sales in European Union for each publisher\n",
    "bestEUPublisherEu = video_game_Year.groupBy('Publisher').agg(\n",
    "    sum(col('EU_Sales')).alias('EU_Total')\n",
    ")\n",
    "\n",
    "# Find the publisher with the highest total sales in European Union\n",
    "bestEUPublisher = bestEUPublisherEu.orderBy('EU_Total', ascending=False).first()['Publisher']\n",
    "\n",
    "# Filter for the best European Union publisher\n",
    "bestEUPublisher_match = video_game_Year.filter(video_game_Year['Publisher'] == bestEUPublisher)\n",
    "\n",
    "# Best publisher sales by Year\n",
    "bestEUPublisherSales = bestEUPublisher_match.groupBy('Year').agg(\n",
    "    round(sum('EU_Sales'), 2).alias('EU_Total'),\n",
    "    round(sum('Global_Sales'), 2).alias('Global_Total')\n",
    ").orderBy('Year')\n",
    "\n",
    "print(f\"The publisher with the highest total video game sales in European Union is: '{bestEUPublisher}'\")\n",
    "print(\"Sales data for the publisher:\")\n",
    "bestEUPublisherSales.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a0e52d-3269-4c61-a5a8-42308ff7b9ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1279180\n+------+-------+------------+------------+-------------+-------------+-------------+----+-----+----+\n|season|game_id|homeTeamCode|awayTeamCode|isPlayOffGame|homeTeamGoals|awayTeamGoals|time|event|team|\n+------+-------+------------+------------+-------------+-------------+-------------+----+-----+----+\n|  2016|  20702|         PIT|         BOS|            0|            0|            0| 920| MISS|AWAY|\n|  2021|  20358|         NSH|         BOS|            0|            0|            2|3177| SHOT|AWAY|\n|  2013|  20405|         DAL|         EDM|            0|            1|            0|2117| SHOT|AWAY|\n|  2021|  20136|         MTL|         DET|            0|            2|            0|3024| SHOT|HOME|\n|  2019|  20856|         BUF|         ANA|            0|            0|            1| 569| SHOT|HOME|\n+------+-------+------------+------------+-------------+-------------+-------------+----+-----+----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Basic Task 2\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"shotID\", IntegerType(), True),\n",
    "    StructField(\"homeTeamCode\", StringType(), True),\n",
    "    StructField(\"awayTeamCode\", StringType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"isPlayOffGame\", IntegerType(), True),\n",
    "    StructField(\"game_id\", IntegerType(), True),\n",
    "    StructField(\"time\", IntegerType(), True),\n",
    "    StructField(\"period\", IntegerType(), True),\n",
    "    StructField(\"team\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"event\", StringType(), True),\n",
    "    StructField(\"homeTeamGoals\", IntegerType(), True),\n",
    "    StructField(\"awayTeamGoals\", IntegerType(), True),\n",
    "    StructField(\"homeTeamWon\", IntegerType(), True),\n",
    "    StructField(\"shotType\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"NHLShots\").getOrCreate()\n",
    "\n",
    "# Read the parquet file with the specified schema\n",
    "shotsDF = spark.read.schema(schema).parquet(\"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/nhl_shots.parquet\") \\\n",
    "    .select(\"season\", \"game_id\", \"homeTeamCode\", \"awayTeamCode\", \"isPlayOffGame\", \"homeTeamGoals\", \n",
    "            \"awayTeamGoals\", \"time\", \"event\", \"team\")\n",
    "\n",
    "# Cache the DataFrame\n",
    "shotsDF.cache()\n",
    "\n",
    "# Display the number of rows and show the first 5 rows\n",
    "print(\"Number of rows:\", shotsDF.count())\n",
    "shotsDF.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990b5053-318e-4ea1-93db-454cbde12839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 15079\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|season|game_id|homeTeamCode|awayTeamCode|isPlayOffGame|homeTeamGoals|awayTeamGoals|lastGoalTime|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|  2021|  20358|         NSH|         BOS|            0|            0|            2|        3599|\n|  2019|  20108|         PIT|         DAL|            0|            4|            2|        3588|\n|  2011|  30183|         DET|         NSH|            1|            2|            3|        3546|\n|  2012|  20587|         WPG|         BUF|            0|            4|            1|        3574|\n|  2017|  20952|         N.J|         NYI|            0|            2|            1|        3555|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\nonly showing top 5 rows\n\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|season|game_id|homeTeamCode|awayTeamCode|isPlayOffGame|homeTeamGoals|awayTeamGoals|lastGoalTime|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n|  2012|  20556|         COL|         DET|            0|            2|            3|        3884|\n+------+-------+------------+------------+-------------+-------------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Basic Task 3\n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/nhl_shots.parquet\"\n",
    "# Read data into shotsDF DataFrame\n",
    "\n",
    "shotsDF = spark.read.schema(schema).parquet(file_path)\n",
    "\n",
    "# Create a temporary view for the shots DataFrame\n",
    "shotsDF.createOrReplaceTempView(\"shots\")\n",
    "\n",
    "# Define the games DataFrame using Spark SQL\n",
    "gamesDF = shotsDF.groupBy(\n",
    "    \"season\", \"game_id\", \"homeTeamCode\", \"awayTeamCode\", \"isPlayOffGame\"\n",
    ").agg(\n",
    "    F.sum(F.when((F.col(\"team\") == 'HOME') & (F.col(\"event\") == 'GOAL'), 1).otherwise(0)).alias(\"homeTeamGoals\"),\n",
    "    F.sum(F.when((F.col(\"team\") == 'AWAY') & (F.col(\"event\") == 'GOAL'), 1).otherwise(0)).alias(\"awayTeamGoals\"),\n",
    "    F.max(F.when((F.col(\"homeTeamGoals\") > 0) | (F.col(\"awayTeamGoals\") > 0), F.col(\"time\"))).alias(\"lastGoalTime\")\n",
    ")\n",
    "\n",
    "# Cache the DataFrame\n",
    "gamesDF.cache()\n",
    "\n",
    "# Display the number of rows and show the first 5 rows\n",
    "print(\"Number of rows:\", gamesDF.count())\n",
    "gamesDF.show(5)\n",
    "\n",
    "# Checking out the output for a specific game\n",
    "filtered_game = gamesDF.filter(\n",
    "    (gamesDF['game_id'] == 20556) &\n",
    "    (gamesDF['homeTeamCode'] == 'COL') &\n",
    "    (gamesDF['awayTeamCode'] == 'DET')\n",
    ")\n",
    "\n",
    "# Display the filtered game\n",
    "filtered_game.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a603d9c-ee49-4ff3-adbb-0e1b8bb6a1b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+\n|season|teamCode|games|wins|losses|\n+------+--------+-----+----+------+\n|  2012|     PIT|   15|   8|     7|\n|  2011|     NYR|   20|  10|    10|\n|  2020|     CAR|   11|   5|     6|\n|  2021|     CAR|   14|   7|     7|\n|  2012|     OTT|   10|   5|     5|\n|  2011|     DET|    5|   1|     4|\n|  2014|     CHI|   23|  16|     7|\n|  2020|     T.B|   23|  16|     7|\n|  2019|     FLA|    4|   1|     3|\n|  2012|     MIN|    5|   1|     4|\n+------+--------+-----+----+------+\nonly showing top 10 rows\n\n+------+--------+-----+----+------+\n|season|teamCode|games|wins|losses|\n+------+--------+-----+----+------+\n|  2017|     BOS|   12|   5|     7|\n+------+--------+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Basic Task 4\n",
    "\n",
    "\n",
    "# Filter playoff games\n",
    "playoffGamesDF = gamesDF.filter(col(\"isPlayOffGame\") == 1)\n",
    "\n",
    "# Define win and loss columns for home and away teams\n",
    "winsLossesDF = playoffGamesDF.withColumn(\"homeWin\", when(col(\"homeTeamGoals\") > col(\"awayTeamGoals\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"homeLoss\", when(col(\"homeTeamGoals\") < col(\"awayTeamGoals\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"awayWin\", when(col(\"awayTeamGoals\") > col(\"homeTeamGoals\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"awayLoss\", when(col(\"awayTeamGoals\") < col(\"homeTeamGoals\"), 1).otherwise(0))\n",
    "\n",
    "# Aggregate wins and losses for home teams\n",
    "homeWinsLossesDF = winsLossesDF.groupBy(\"season\", \"homeTeamCode\") \\\n",
    "    .agg(\n",
    "        sum(\"homeWin\").alias(\"wins\"),\n",
    "        sum(\"homeLoss\").alias(\"losses\"),\n",
    "        count(\"game_id\").alias(\"games\")\n",
    "    ) \\\n",
    "    .withColumnRenamed(\"homeTeamCode\", \"teamCode\")\n",
    "\n",
    "# Aggregate wins and losses for away teams\n",
    "awayWinsLossesDF = winsLossesDF.groupBy(\"season\", \"awayTeamCode\") \\\n",
    "    .agg(\n",
    "        sum(\"awayWin\").alias(\"wins\"),\n",
    "        sum(\"awayLoss\").alias(\"losses\"),\n",
    "        count(\"game_id\").alias(\"games\")\n",
    "    ) \\\n",
    "    .withColumnRenamed(\"awayTeamCode\", \"teamCode\")\n",
    "\n",
    "# Union home and away results, then aggregate for each team and season\n",
    "playoffDF = homeWinsLossesDF.union(awayWinsLossesDF) \\\n",
    "    .groupBy(\"season\", \"teamCode\") \\\n",
    "    .agg(\n",
    "        sum(\"games\").alias(\"games\"),\n",
    "        sum(\"wins\").alias(\"wins\"),\n",
    "        sum(\"losses\").alias(\"losses\")\n",
    "    )\n",
    "\n",
    "playoffDF.show(10)\n",
    "\n",
    "filtered_game = playoffDF.filter(\n",
    "    (playoffDF['season'] == 2017) &\n",
    "    (playoffDF['teamCode'] == 'BOS') )\n",
    "    \n",
    "\n",
    "filtered_game.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c8d815-18a0-452e-ad20-cec802c053bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+\n|season|teamCode|games|wins|losses|\n+------+--------+-----+----+------+\n|  2011|     L.A|   20|  16|     4|\n|  2012|     CHI|   23|  15|     7|\n|  2013|     L.A|   26|  16|    10|\n|  2014|     CHI|   23|  16|     7|\n|  2015|     PIT|   24|  16|     8|\n|  2016|     PIT|   25|  16|     9|\n|  2017|     WSH|   24|  16|     8|\n|  2018|     STL|   26|  16|    10|\n|  2019|     T.B|   25|  18|     7|\n|  2020|     T.B|   23|  16|     7|\n|  2021|     COL|   20|  16|     4|\n|  2022|     VGK|   22|  16|     6|\n+------+--------+-----+----+------+\n\nBest Team of 2022\nRow(season=2022, teamCode='VGK', games=22, wins=16, losses=6)\n"
     ]
    }
   ],
   "source": [
    "# Basic Task 5\n",
    "relevantPlayoffDF = playoffDF.select(\"season\", \"teamCode\", \"games\", \"wins\", \"losses\")\n",
    "\n",
    "# Define a window specification\n",
    "windowSpec = Window().partitionBy(\"season\").orderBy(F.col(\"wins\").desc())\n",
    "\n",
    "# Use row_number() over the window to rank teams\n",
    "bestPlayoffTeams = relevantPlayoffDF.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "    .filter(F.col(\"row\") == 1) \\\n",
    "    .drop(\"row\")\n",
    "\n",
    "bestPlayoffTeams.show()\n",
    "bestPlayoffTeam2022: Row = bestPlayoffTeams.filter(\"season == 2022\").first()\n",
    "print('Best Team of 2022')\n",
    "print(bestPlayoffTeam2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842ebc68-2a0a-4013-83d0-6bc77251e032",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2020|     CAR|   56|  31|    25|        175|          133|   102|\n|  2015|     COL|   82|  35|    47|        212|          240|   110|\n|  2019|     VAN|   69|  31|    38|        223|          211|    99|\n|  2016|     PIT|   82|  46|    36|        277|          228|   146|\n|  2017|     PIT|   82|  45|    37|        269|          248|   131|\n+------+--------+-----+----+------+-----------+-------------+------+\nonly showing top 5 rows\n\n+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2022|     NYI|   82|  41|    41|        242|          217|   128|\n+------+--------+-----+----+------+-----------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Basic Task 6\n",
    "\n",
    "\n",
    "nonPlayOffDF = gamesDF.filter(col(\"isPlayOffGame\") == 0)\n",
    "\n",
    "teamGoalsDF = nonPlayOffDF.select(\n",
    "    \"season\", nonPlayOffDF[\"homeTeamCode\"].alias(\"teamCode\"), nonPlayOffDF[\"homeTeamGoals\"].alias(\"goalsScored\"), nonPlayOffDF[\"awayTeamGoals\"].alias(\"goalsConceded\"), \"lastGoalTime\"\n",
    ").union(\n",
    "    nonPlayOffDF.select(\n",
    "        \"season\", nonPlayOffDF[\"awayTeamCode\"].alias(\"teamCode\"), nonPlayOffDF[\"awayTeamGoals\"].alias(\"goalsScored\"), nonPlayOffDF[\"homeTeamGoals\"].alias(\"goalsConceded\"), \"lastGoalTime\"\n",
    "    )\n",
    ")\n",
    "\n",
    "teamGoalsDF = teamGoalsDF.withColumn(\n",
    "    \"points\",\n",
    "    when((col(\"goalsScored\") > col(\"goalsConceded\")) & (col(\"lastGoalTime\") <= 3600), 3)\n",
    "    .when(((col(\"goalsScored\") < col(\"goalsConceded\")) & (col(\"lastGoalTime\") > 3600)), 1)\n",
    "    .when((col(\"goalsScored\") == col(\"goalsConceded\")), 1)\n",
    "    .when((col(\"goalsScored\") > col(\"goalsConceded\")) & (col(\"lastGoalTime\") > 3600), 2)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "teamResultsDF = teamGoalsDF.withColumn(\n",
    "    \"wins\", when(col(\"points\") >= 2, 1).otherwise(0)\n",
    ").withColumn( \n",
    "    \"losses\", when((col(\"points\") <= 1), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "regularSeasonDF = teamResultsDF.groupBy(\"season\", \"teamCode\").agg(\n",
    "    expr(\"count(*) as games\"),\n",
    "    expr(\"sum(wins) as wins\"),\n",
    "    expr(\"sum(losses) as losses\"),\n",
    "    expr(\"sum(goalsScored) as goalsScored\"),\n",
    "    expr(\"sum(goalsConceded) as goalsConceded\"),\n",
    "    expr(\"sum(points) as points\")\n",
    ")\n",
    "\n",
    "regularSeasonDF.show(5)\n",
    "\n",
    "XDF = regularSeasonDF.filter(\n",
    "    (regularSeasonDF['season'] == 2022) &\n",
    "    (regularSeasonDF['teamCode'] == 'NYI') )\n",
    "\n",
    "XDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d71af1-9489-45e1-a7df-910197689924",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2011|     CBJ|   82|  25|    57|        197|          257|    84|\n|  2012|     FLA|   48|  12|    36|        107|          170|    43|\n|  2013|     BUF|   82|  14|    68|        149|          242|    55|\n|  2014|     BUF|   82|  15|    67|        153|          268|    60|\n|  2015|     TOR|   82|  23|    59|        192|          240|    82|\n|  2016|     COL|   82|  21|    61|        165|          276|    61|\n|  2017|     BUF|   82|  24|    58|        198|          277|    80|\n|  2018|     OTT|   82|  29|    53|        243|          299|    88|\n|  2019|     DET|   71|  14|    57|        142|          265|    49|\n|  2020|     BUF|   56|  11|    45|        133|          196|    44|\n|  2021|     MTL|   82|  19|    63|        216|          316|    68|\n|  2022|     ANA|   82|  20|    62|        205|          335|    68|\n+------+--------+-----+----+------+-----------+-------------+------+\n\n+------+--------+-----+----+------+-----------+-------------+------+\n|season|teamCode|games|wins|losses|goalsScored|goalsConceded|points|\n+------+--------+-----+----+------+-----------+-------------+------+\n|  2022|     ANA|   82|  20|    62|        205|          335|    68|\n+------+--------+-----+----+------+-----------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Basic Tassk 7\n",
    "\n",
    "\n",
    "\n",
    "worstRegularTeams = (\n",
    "    regularSeasonDF\n",
    "    .withColumn(\"rank\", dense_rank().over(Window.partitionBy(\"season\").orderBy(\"points\")))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "# Show the worst regular season teams for each season\n",
    "worstRegularTeams.show()\n",
    "# Fetch details for the worst regular season team in 2022\n",
    "worstRegularTeams2022 = worstRegularTeams.filter(\"season == 2022\")\n",
    "\n",
    "# Show the worst regular season for 2022\n",
    "worstRegularTeams2022.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539a41b9-a913-4c7a-95b6-4523f5b64f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional Task 2 - Unstructured data (2 points)\n",
    "\n",
    "You are given some text files with contents from a few thousand random articles both in English and Finnish from Wikipedia. Content from English articles are in the [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) at folder `assignment/wikipedia/en` and content from Finnish articles are at folder `assignment/wikipedia/fi`.\n",
    "\n",
    "Some cleaning operations have already been done to the texts but the some further cleaning is still required.\n",
    "\n",
    "The final goal of the task is to get the answers to following questions:\n",
    "\n",
    "- What are the ten most common English words that appear in the English articles?\n",
    "- What are the five most common 5-letter Finnish words that appear in the Finnish articles?\n",
    "- What is the longest word that appears at least 150 times in the articles?\n",
    "- What is the average English word length for the words appearing in the English articles?\n",
    "- What is the average Finnish word length for the words appearing in the Finnish articles?\n",
    "\n",
    "For a word to be included in the calculations, it should fulfill the following requirements:\n",
    "\n",
    "- Capitalization is to be ignored. I.e., words \"English\", \"ENGLISH\", and \"english\" are all to be considered as the same word \"english\".\n",
    "- An English word should only contain the 26 letters from the alphabet of Modern English. Only exception is that punctuation marks, i.e. hyphens `-`, are allowed in the middle of the words as long as there are no two consecutive punctuation marks without any letters between them.\n",
    "- The only allowed 1-letter English words are `a` and `i`.\n",
    "- A Finnish word should follow the same rules as English words, except that three additional letters, ``, ``, and ``, are also allowed, and that no 1-letter words are allowed. Also, any word that contains \"`wiki`\" should not be considered as Finnish word.\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- Using an RDD or a Dataset (in Scala) might make the data cleaning and word determination easier than using DataFrames.\n",
    "- It can be assumed that in the source data each word in the same line is separated by at least one white space (` `).\n",
    "- You are allowed to remove all non-allowed characters from the source data at the beginning of the cleaning process.\n",
    "- It is advisable to first create a DataFrame/Dataset/RDD that contains the found words, their language, and the number of times those words appeared in the articles. This can then be used as the starting point when determining the answers to the given questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e53f397-4a27-4cd3-84d3-d32786cbfde6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most common English words that appear in the English articles:\n+-----+------+\n|words|count |\n+-----+------+\n|the  |161750|\n|of   |85859 |\n|and  |68285 |\n|in   |63339 |\n|to   |48648 |\n|was  |22870 |\n|by   |18996 |\n|as   |18734 |\n|for  |17209 |\n|is   |17049 |\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, udf, explode, split, length, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "from typing import List\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WikipediaAnalysis\").getOrCreate()\n",
    "\n",
    "# Constants\n",
    "allowedOneLetterWords: List[str] = [\"a\", \"i\"]\n",
    "englishLetters: str = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "finnishLetters: str = englishLetters + \"\"\n",
    "whiteSpace: str = \" \"\n",
    "punctuationMark: str = '-'\n",
    "allowedEnglishOneLetterWords: List[str] = [\"a\", \"i\"]\n",
    "wikiStr: str = \"wiki\"\n",
    "\n",
    "# Sample paths\n",
    "english_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/wikipedia/en/*.txt\"\n",
    "finnish_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/wikipedia/fi/*.txt\"\n",
    "\n",
    "# Read text files into RDDs\n",
    "english_rdd = spark.sparkContext.textFile(english_path)\n",
    "finnish_rdd = spark.sparkContext.textFile(finnish_path)\n",
    "\n",
    "# Function to clean and filter words\n",
    "def clean_and_filter_words(text: str, allowed_letters: set, one_letter_words: List[str]) -> list:\n",
    "    cleaned_text = ''.join(char.lower() if char.lower() in allowed_letters or char == ' ' else ' ' for char in text)\n",
    "    words = cleaned_text.split()\n",
    "    translator = str.maketrans(\"\", \"\", f\"{whiteSpace}{punctuationMark}\")\n",
    "    filtered_words = [word.strip().translate(translator) for word in words if len(word) > 1 and word not in one_letter_words]\n",
    "    return filtered_words\n",
    "\n",
    "# Register UDFs\n",
    "clean_and_filter_english_udf = udf(lambda x: clean_and_filter_words(x, set(englishLetters), allowedEnglishOneLetterWords), StringType())\n",
    "clean_and_filter_finnish_udf = udf(lambda x: clean_and_filter_words(x, set(finnishLetters), []), StringType())\n",
    "\n",
    "# Apply UDFs to RDDs and create DataFrames\n",
    "commonWordsEn = (\n",
    "    english_rdd\n",
    "    .map(lambda x: (x,))\n",
    "    .toDF([\"text\"])\n",
    "    .withColumn(\"words\", explode(split(clean_and_filter_english_udf(\"text\"), \" \")))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \",\", \"\"))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \"[\\[\\]]\", \"\"))\n",
    "    .groupBy(\"words\")\n",
    "    .count()\n",
    "    .filter((col(\"words\").isin(allowedEnglishOneLetterWords) | (length(col(\"words\")) > 1)) & (col(\"words\") != \"\"))\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"The ten most common English words that appear in the English articles:\")\n",
    "commonWordsEn.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce210f0-5a6a-48f2-b610-1974f43f4d78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most common 5-letter Finnish words that appear in the Finnish articles:\n+-----+-----+\n|words|count|\n+-----+-----+\n|mutta| 3567|\n|luvun| 2086|\n|hnen| 2054|\n|jonka| 1873|\n|jossa| 1635|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "common5LetterWordsFi = (\n",
    "    finnish_rdd\n",
    "    .map(lambda x: (x,))\n",
    "    .toDF([\"text\"])\n",
    "    .withColumn(\"words\", explode(split(clean_and_filter_finnish_udf(\"text\"), \" \")))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \",\", \"\"))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \"[\\[\\]]\", \"\"))\n",
    "    .groupBy(\"words\")\n",
    "    .count()\n",
    "    .filter((col(\"words\").isin(allowedOneLetterWords) | (length(col(\"words\")) == 5)) & (col(\"words\") != \"\"))\n",
    "    .orderBy(col(\"count\").desc())\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"The five most common 5-letter Finnish words that appear in the Finnish articles:\")\n",
    "common5LetterWordsFi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c10a5a2-c0c5-4339-80a7-f72360d6118c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest word appearing at least 150 times is 'yhdysvaltalainen'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "allWordsEn = (\n",
    "    english_rdd\n",
    "    .map(lambda x: (x,))\n",
    "    .toDF([\"text\"])\n",
    "    .withColumn(\"words\", explode(split(clean_and_filter_english_udf(\"text\"), \" \")))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \",\", \"\"))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \"[\\[\\]]\", \"\"))\n",
    "    .filter(length(col(\"words\")) > 0)  # Filter out empty strings\n",
    "    .select(\"words\")\n",
    ")\n",
    "\n",
    "allWordsFi = (\n",
    "    finnish_rdd\n",
    "    .map(lambda x: (x,))\n",
    "    .toDF([\"text\"])\n",
    "    .withColumn(\"words\", explode(split(clean_and_filter_finnish_udf(\"text\"), \" \")))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \",\", \"\"))\n",
    "    .withColumn(\"words\", regexp_replace(col(\"words\"), \"[\\[\\]]\", \"\"))\n",
    "    .filter(length(col(\"words\")) > 0)  # Filter out empty strings\n",
    "    .select(\"words\")\n",
    ")\n",
    "\n",
    "# Combine English and Finnish words\n",
    "combinedWords = allWordsEn.union(allWordsFi)\n",
    "\n",
    "# Find the longest word appearing at least 150 times\n",
    "result = (\n",
    "    combinedWords\n",
    "    .groupBy(\"words\")\n",
    "    .count()\n",
    "    .filter(col(\"count\") >= 150)\n",
    "    .orderBy(length(col(\"words\")).desc())\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "# Extract the word from the DataFrame\n",
    "longest_word_row = result.select(\"words\").first()\n",
    "longest_word = longest_word_row[\"words\"]\n",
    "print(f\"The longest word appearing at least 150 times is '{longest_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40bd663c-7d2f-4d00-87e7-d629055d555c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average word lengths:\n+--------+--------------+\n|Language|Average_Length|\n+--------+--------------+\n| Finnish|          7.69|\n| English|          5.25|\n+--------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length, avg\n",
    "\n",
    "# Calculate average word lengths for English\n",
    "average_word_lengths_english = (\n",
    "    allWordsEn\n",
    "    .groupBy()\n",
    "    .agg(avg(length(col(\"words\"))).alias(\"average_length_english\"))\n",
    ")\n",
    "\n",
    "# Calculate average word lengths for Finnish\n",
    "average_word_lengths_finnish = (\n",
    "    allWordsFi\n",
    "    .groupBy()\n",
    "    .agg(avg(length(col(\"words\"))).alias(\"average_length_finnish\"))\n",
    ")\n",
    "\n",
    "# Create a new DataFrame with the information\n",
    "result_data = [\n",
    "    (\"Finnish\", \"{:.2f}\".format(average_word_lengths_finnish.first()[\"average_length_finnish\"])),\n",
    "    (\"English\", \"{:.2f}\".format(average_word_lengths_english.first()[\"average_length_english\"]))\n",
    "]\n",
    "\n",
    "result_columns = [\"Language\", \"Average_Length\"]\n",
    "\n",
    "averageWordLengths = spark.createDataFrame(result_data, result_columns)\n",
    "\n",
    "print(\"The average word lengths:\")\n",
    "averageWordLengths.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8619043c-6b9e-4801-86c4-188bad80118c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional Task 3 - K-Means clustering (2 points)\n",
    "\n",
    "You are given a dataset containing the locations of building in Finland. The dataset is a subset from [https://www.avoindata.fi/data/en_GB/dataset/postcodes/resource/3c277957-9b25-403d-b160-b61fdb47002f](https://www.avoindata.fi/data/en_GB/dataset/postcodes/resource/3c277957-9b25-403d-b160-b61fdb47002f) limited to only postal codes with the first two numbers in the interval 30-44 ([postal codes in Finland](https://www.posti.fi/en/zip-code-search/postal-codes-in-finland)). The dataset is in the [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) at folder `assignment/buildings.parquet`.\n",
    "\n",
    "[K-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering) algorithm is an unsupervised machine learning algorithm that can be used to partition the input data into k clusters. Your task is to use the Spark ML library and its K-Means clusterization algorithm to divide the buildings into clusters using the building coordinates `latitude_wgs84` and `longitude_wgs84` as the basis of the clusterization. You should implement the following procedure:\n",
    "\n",
    "1. Start with all the buildings in the dataset.\n",
    "2. Divide the buildings into seven clusters with K-Means algorithm using `k=7` and the longitude and latitude of the buildings.\n",
    "3. Find the cluster to which the Shktalo building from the Hervanta campus is sorted into. The building id for Shktalo in the dataset is `102363858X`.\n",
    "4. Choose all the buildings from the cluster with the Shktalo building.\n",
    "5. Find the cluster center for the chosen cluster of buildings.\n",
    "6. Calculate the largest distance from a building in the chosen cluster to the chosen cluster center. You are given a function `haversine` that you can use to calculate the distance between two points using the latitude and longitude of the points.\n",
    "7. While the largest distance from a building in the considered cluster to the cluster center is larger than 3 kilometers run the K-Means algorithm again using the following substeps.\n",
    "    - Run the K-Means algorithm to divide the remaining buildings into smaller clusters. The number of the new clusters should be one less than in the previous run of the algorithm (but should always be at least two). I.e., the sequence of `k` values starting from the second run should be 6, 5, 4, 3, 2, 2, ...\n",
    "    - After using the algorithm again, choose the new smaller cluster of buildings so that it includes the Shktalo building.\n",
    "    - Find the center of this cluster and calculate the largest distance from a building in this cluster to its center.\n",
    "\n",
    "As the result of this process, you should get a cluster of buildings that includes the Shktalo building and in which all buildings are within 3 kilometers of the cluster center.\n",
    "\n",
    "Using the final cluster, find the answers to the following questions:\n",
    "\n",
    "- How many buildings in total are in the final cluster?\n",
    "- How many Hervanta buildings are in this final cluster? (A building is considered to be in Hervanta if their postal code is `33720`)\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- Once you have trained a KMeansModel, the coordinates for the cluster centers, and the cluster indexes for individual buildings can be accessed through the model object (`clusterCenters`, `summary.predictions`).\n",
    "- The given haversine function for calculating distances can be used with data frames if you turn it into an user defined function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc6d3ee-136f-478c-bd5d-2d51dbe41471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "startK: int = 7\n",
    "seedValue: int = 1\n",
    "\n",
    "# the building id for Shktalo building at Hervanta campus\n",
    "hervantaBuildingId: str = \"102363858X\"\n",
    "hervantaPostalCode: int = 33720\n",
    "\n",
    "maxAllowedClusterDistance: float = 3.0\n",
    "\n",
    "\n",
    "# returns the distance between points (lat1, lon1) and (lat2, lon2) in kilometers\n",
    "# based on https://community.esri.com/t5/coordinate-reference-systems-blog/distance-on-a-sphere-the-haversine-formula/ba-p/902128\n",
    "def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    R: float = 6378.1  # radius of Earth in kilometers\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    deltaPhi = math.radians(lat2 - lat1)\n",
    "    deltaLambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = (\n",
    "        math.sin(deltaPhi * deltaPhi / 4.0) +\n",
    "        math.cos(phi1) * math.cos(phi2) * math.sin(deltaLambda * deltaLambda / 4.0)\n",
    "    )\n",
    "\n",
    "    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b863a2c-8909-4c62-90d6-eac53ab03269",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(k=7, iteration=1) Buildings: 343556 -> 84990, Maximum distance to the center within 'Shktalo' cluster: 82.68 km\n(k=6, iteration=2) Buildings: 84990 -> 25299, Maximum distance to the center within 'Shktalo' cluster: 33.60 km\n(k=5, iteration=3) Buildings: 25299 -> 10581, Maximum distance to the center within 'Shktalo' cluster: 12.15 km\n(k=4, iteration=4) Buildings: 10581 -> 2304, Maximum distance to the center within 'Shktalo' cluster: 10.81 km\n(k=3, iteration=5) Buildings: 2304 -> 809, Maximum distance to the center within 'Shktalo' cluster: 10.81 km\n(k=2, iteration=6) Buildings: 809 -> 757, Maximum distance to the center within 'Shktalo' cluster: 4.52 km\nExiting due to negligible distance reduction.\nBuildings in the final cluster: 757\nHervanta buildings in the final cluster: 669 (88.38% of all buildings in the final cluster)\n===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType\n",
    "import math\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BuildingClustering\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/assignment/buildings.parquet\"\n",
    "buildings_df = spark.read.parquet(file_path)\n",
    "\n",
    "# Define the Haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6378.1  # radius of Earth in kilometers\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    deltaPhi = math.radians(lat2 - lat1)\n",
    "    deltaLambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = (\n",
    "        math.sin(deltaPhi * deltaPhi / 4.0) +\n",
    "        math.cos(phi1) * math.cos(phi2) * math.sin(deltaLambda * deltaLambda / 4.0)\n",
    "    )\n",
    "\n",
    "    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "# Define Haversine as a user-defined function\n",
    "@udf(FloatType())\n",
    "def haversine_udf(lat, lon):\n",
    "    return haversine(lat, lon, cluster_center[0], cluster_center[1])\n",
    "\n",
    "# some helpful constants\n",
    "startK: int = 7\n",
    "seedValue: int = 1\n",
    "\n",
    "# the building id for Shktalo building at Hervanta campus\n",
    "hervantaBuildingId: str = \"102363858X\"\n",
    "hervantaPostalCode: int = 33720\n",
    "\n",
    "maxAllowedClusterDistance: float = 3.0\n",
    "\n",
    "# Create a VectorAssembler to assemble the features\n",
    "vec_assembler = VectorAssembler(inputCols=[\"latitude_wgs84\", \"longitude_wgs84\"], outputCol=\"features\")\n",
    "buildings_df = vec_assembler.transform(buildings_df)\n",
    "\n",
    "# Initialize variables\n",
    "k = startK\n",
    "iteration = 1\n",
    "\n",
    "# Placeholder for the previous cluster center and selected cluster df\n",
    "prev_cluster_center = None\n",
    "prev_selected_cluster_df = None\n",
    "\n",
    "while True:\n",
    "    # Use K-Means algorithm\n",
    "    kmeans = KMeans(featuresCol=\"features\", k=k, seed=seedValue)\n",
    "    model = kmeans.fit(buildings_df)\n",
    "\n",
    "    # Use previous values for the first iteration\n",
    "    if prev_cluster_center is None:\n",
    "        prev_cluster_center = model.clusterCenters()[0]\n",
    "        prev_selected_cluster_df = buildings_df\n",
    "\n",
    "    predictions_col = f\"prediction_{iteration}\"\n",
    "    predictions = model.transform(buildings_df).withColumnRenamed(\"prediction\", predictions_col)\n",
    "\n",
    "    # Find the cluster to which the Shktalo building from the Hervanta campus is sorted into\n",
    "    sahkotalo_cluster = predictions.filter(col(\"building_id\") == hervantaBuildingId).select(predictions_col).first()[predictions_col]\n",
    "\n",
    "    # Choose all the buildings from the cluster with the Shktalo building\n",
    "    selected_cluster_df = predictions.filter(col(predictions_col) == sahkotalo_cluster)\n",
    "\n",
    "    # Find the center of this cluster\n",
    "    cluster_center = model.clusterCenters()[sahkotalo_cluster]\n",
    "    max_distance = (\n",
    "    selected_cluster_df\n",
    "    .withColumn(\"distance\", haversine_udf(col(\"latitude_wgs84\"), col(\"longitude_wgs84\")))\n",
    "    .groupBy(predictions_col)\n",
    "    .agg({\"distance\": \"max\"})\n",
    "    .first()[\"max(distance)\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Display the cluster information for each iteration\n",
    "    print(f\"(k={k}, iteration={iteration}) Buildings: {prev_selected_cluster_df.count()} -> {selected_cluster_df.count()}, Maximum distance to the center within 'Shktalo' cluster: {max_distance:.2f} km\")\n",
    "\n",
    "    # Check if the condition is met\n",
    "    if max_distance <= maxAllowedClusterDistance or k == 2:\n",
    "        print(f\"Exiting due to negligible distance reduction.\")\n",
    "        break\n",
    "\n",
    "    # Update variables for the next iteration\n",
    "    k -= 1\n",
    "    iteration += 1\n",
    "    prev_max_distance = max_distance\n",
    "    prev_cluster_center = cluster_center\n",
    "    prev_selected_cluster_df = selected_cluster_df\n",
    "    buildings_df = selected_cluster_df  # Update the dataset for the next iteration\n",
    "\n",
    "# Display the final cluster of buildings\n",
    "finalCluster = selected_cluster_df\n",
    "\n",
    "# Calculate counts\n",
    "clusterBuildingCount: int = finalCluster.count()\n",
    "clusterHervantaBuildingCount: int = finalCluster.filter(col(\"postal_code\") == hervantaPostalCode).count()\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Buildings in the final cluster: {clusterBuildingCount}\")\n",
    "print(f\"Hervanta buildings in the final cluster: {clusterHervantaBuildingCount} \", end=\"\")\n",
    "print(f\"({round(100*clusterHervantaBuildingCount/clusterBuildingCount, 2)}% of all buildings in the final cluster)\")\n",
    "print(\"===========================================================================================\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_group assignment_Dataa",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
