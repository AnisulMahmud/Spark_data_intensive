{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8e27ee-a2a4-4120-bbb9-05d81ba2ddcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# COMP.CS.320 Data-Intensive Programming, Exercise 3\n",
    "\n",
    "This exercise has three parts:\n",
    "\n",
    "- tasks 1-3 concern data queries for static data\n",
    "- tasks 4-5 are examples of using typed Dataset instead of DataFrame\n",
    "- tasks 6-8 concern the same data query as in the first two tasks but handled as streaming data\n",
    "\n",
    "The tasks can be done in either Scala or Python. This is the **Python** version, switch to the Scala version if you want to do the tasks in Scala.\n",
    "\n",
    "Each task has its own cell for the code. Add your solutions to the cells. You are free to add more cells if you feel it is necessary. There are cells with example outputs or test code following most of the tasks.\n",
    "\n",
    "Don't forget to submit your solutions to Moodle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe88dd46-4491-4a78-b5f0-020feaa1022e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# some imports that might be required in the tasks\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3af6c7-214c-4af4-914c-ca857a075b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#some more imports \n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as Func, DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24018d6-6a96-4cc5-bc7a-4720385c8dd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 1 - Create a DataFrame for retailer data\n",
    "\n",
    "In the [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) in the `exercises/ex3` folder is file `sales_data_sample.csv` that contains retaier sales data in CSV format.\n",
    "The direct address for the data file is: `abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex3/sales_data_sample.csv`\n",
    "\n",
    "Read the data from the CSV file into DataFrame called retailerDataFrame. Let Spark infer the schema for the data. Note, that this CSV file uses semicolons (`;`) as the column separator instead of the default commas (`,`).\n",
    "\n",
    "Print out the schema, the resulting DataFrame should have 24 columns. The data contains information about the item price and the number of items ordered for each day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d20a22-516e-4c13-b889-d675f3ab29f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ORDERNUMBER: integer (nullable = true)\n |-- QUANTITYORDERED: integer (nullable = true)\n |-- PRICEEACH: double (nullable = true)\n |-- ORDERLINENUMBER: integer (nullable = true)\n |-- ORDERDATE: date (nullable = true)\n |-- STATUS: string (nullable = true)\n |-- QTR_ID: integer (nullable = true)\n |-- MONTH_ID: integer (nullable = true)\n |-- YEAR_ID: integer (nullable = true)\n |-- PRODUCTLINE: string (nullable = true)\n |-- MSRP: integer (nullable = true)\n |-- PRODUCTCODE: string (nullable = true)\n |-- CUSTOMERNAME: string (nullable = true)\n |-- PSmallHONE: string (nullable = true)\n |-- ADDRESSLINE1: string (nullable = true)\n |-- ADDRESSLINE2: string (nullable = true)\n |-- CITY: string (nullable = true)\n |-- STATE: string (nullable = true)\n |-- POSTALCODE: string (nullable = true)\n |-- COUNTRY: string (nullable = true)\n |-- TERRITORY: string (nullable = true)\n |-- CONTACTLASTNAME: string (nullable = true)\n |-- CONTACTFIRSTNAME: string (nullable = true)\n |-- DEALSIZE: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
    "\n",
    "# Defining file path \n",
    "file_path = \"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex3/sales_data_sample.csv\"\n",
    "\n",
    "# Reading the  the CSV file as described\n",
    "retailerDataFrame: DataFrame = spark.read.option(\"delimiter\", \";\").csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Printing the schema\n",
    "retailerDataFrame.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa1ad8d-4225-4be8-8273-650113a21d17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 1 (only the first few lines):\n",
    "\n",
    "```text\n",
    "root\n",
    " |-- ORDERNUMBER: integer (nullable = true)\n",
    " |-- QUANTITYORDERED: integer (nullable = true)\n",
    " |-- PRICEEACH: double (nullable = true)\n",
    " |-- ORDERLINENUMBER: integer (nullable = true)\n",
    " |-- ORDERDATE: date (nullable = true)\n",
    " ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb56355-7170-484a-911b-48e5dcead27c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 2 - The best selling days\n",
    "\n",
    "Find the best **12** selling days using the retailer data frame from task 1. That is the days for which `QUANTITYORDERED * PRICEEACH` gets the highest values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bf6c31-ff15-4c27-82df-2bc6c4cd76bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n| ORDERDATE|  sum(total_sales)|\n+----------+------------------+\n|2004-11-24|115033.48000000003|\n|2003-11-14|109509.87999999999|\n|2003-11-12|          90218.58|\n|2003-12-02| 87445.18000000001|\n|2004-10-16|          86320.39|\n|2003-11-06|          84731.32|\n|2004-11-17|          82125.62|\n|2004-11-04|          80807.93|\n|2004-08-20| 80247.84000000001|\n|2004-11-05| 78324.73000000001|\n|2003-11-20|          76973.93|\n|2004-12-10|          76380.08|\n+----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BestSellingDays\").getOrCreate()\n",
    "\n",
    "# Resuming from Task 1\n",
    "\n",
    "# Calculating total sales for each day\n",
    "retailerDataFrame = retailerDataFrame.withColumn(\"total_sales\", col(\"QUANTITYORDERED\") * col(\"PRICEEACH\"))\n",
    "\n",
    "# Group by ORDERDATE and sum the total sales for each day\n",
    "dailySalesDF = retailerDataFrame.groupBy(\"ORDERDATE\").agg({\"total_sales\": \"sum\"})\n",
    "\n",
    "best12DaysDF: DataFrame = dailySalesDF.orderBy(\"sum(total_sales)\", ascending=False).limit(12)\n",
    "\n",
    "# result\n",
    "best12DaysDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e803b507-d2c0-456a-8502-61dc0061a51a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 2:\n",
    "\n",
    "```text\n",
    "+----------+------------------+\n",
    "| ORDERDATE|       total_sales|\n",
    "+----------+------------------+\n",
    "|2004-11-24|115033.48000000003|\n",
    "|2003-11-14|109509.87999999999|\n",
    "|2003-11-12|          90218.58|\n",
    "|2003-12-02| 87445.18000000001|\n",
    "|2004-10-16|          86320.39|\n",
    "|2003-11-06|          84731.32|\n",
    "|2004-11-17|          82125.62|\n",
    "|2004-11-04|          80807.93|\n",
    "|2004-08-20| 80247.84000000001|\n",
    "|2004-11-05| 78324.73000000001|\n",
    "|2003-11-20|          76973.93|\n",
    "|2004-12-10|          76380.08|\n",
    "+----------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c71dce8-8484-4c0f-9d83-3e0e57eb15d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27baf617-f608-4b0b-8aa0-79ddd3b4b6fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 3 - The products with the most sale value\n",
    "\n",
    "Find the product codes for the **8** products that have the most total sale value in year 2003.\n",
    "\n",
    "**Note**, in this task (and only in this task) all sales done in **January** should be counted **twice**.\n",
    "\n",
    "Hint: use the MONTH_ID and YEAR_ID columns to recognize the month and year of each sale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c34d999-4376-4b70-b507-11aff200114e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n|PRODUCTCODE|       total_sales|\n+-----------+------------------+\n|   S18_3232|           69500.0|\n|   S18_2319|           45600.0|\n|   S18_4600|           44400.0|\n|   S50_1392|39813.920000000006|\n|   S18_1342|39661.149999999994|\n|   S12_4473|          39084.36|\n|   S24_3856|           38900.0|\n|   S24_2300|           38800.0|\n+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, sum, when\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TopProducts\").getOrCreate()\n",
    "\n",
    "# Filtering data for the year 2003\n",
    "sales2003DF = retailerDataFrame.filter(col(\"YEAR_ID\") == 2003)\n",
    "\n",
    "# Adjust sales for January (counting twice)\n",
    "sales2003DF = sales2003DF.withColumn(\n",
    "    \"adjusted_sales\",\n",
    "    when(col(\"MONTH_ID\") == 1, col(\"QUANTITYORDERED\") * col(\"PRICEEACH\") * 2)\n",
    "    .otherwise(col(\"QUANTITYORDERED\") * col(\"PRICEEACH\"))\n",
    ")\n",
    "\n",
    "# Group by PRODUCTCODE and sum the adjusted sales for each product\n",
    "productDF: DataFrame = sales2003DF.groupBy(\"PRODUCTCODE\").agg(sum(\"adjusted_sales\").alias(\"total_sales\"))\n",
    "\n",
    "top8ProductsDF = productDF.orderBy(\"total_sales\", ascending=False).limit(8)\n",
    "\n",
    "# result\n",
    "top8ProductsDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be492bc-b356-4f76-b2c1-9df3d61f5fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 3:\n",
    "\n",
    "```text\n",
    "+-----------+------------------+\n",
    "|PRODUCTCODE|       total_sales|\n",
    "+-----------+------------------+\n",
    "|   S18_3232|           69500.0|\n",
    "|   S18_2319|           45600.0|\n",
    "|   S18_4600|           44400.0|\n",
    "|   S50_1392|39813.920000000006|\n",
    "|   S18_1342|39661.149999999994|\n",
    "|   S12_4473|          39084.36|\n",
    "|   S24_3856|           38900.0|\n",
    "|   S24_2300|           38800.0|\n",
    "+-----------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7beaddd-1190-4d8d-8b59-264776aa82f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 4 - Dataset 1\n",
    "\n",
    "This task is originally designed for Scala language. It does not fully translate to Python because Python does not support typed Spark Datasets. Instead of Scala's case class, you can use Python's dataclass.\n",
    "\n",
    "The classes that takes a type parameter are known to be Generic classes in Scala. Dataset is an example of a generic class. Actually, DataFrame is a type alias for Dataset[Row], where Row is the type parameter (Row being general object that can represent any row in a Spark data frame).\n",
    "\n",
    "Declare your own case class (dataclass in Python) Sales with two members: `year` and `euros`, with both being of integer types.\n",
    "\n",
    "Then instantiate a DataFrame of Sales with data from the variable `salesList` and query for the sales on 2017 and for the year with the highest amount of sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672d3b2f-181d-4197-b22e-170ed5ede659",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Sales dataclass in this cell\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesData\").getOrCreate()\n",
    "\n",
    "# Create the Sales dataclass in this cell\n",
    "@dataclass\n",
    "class Sales:\n",
    "    year: int\n",
    "    euros: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325a9927-9476-4b50-b740-dad45cd9869d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales for 2017 is 15\nMaximum sales: year = 2021, euros = 950\n"
     ]
    }
   ],
   "source": [
    "salesList = [Sales(2015, 325), Sales(2016, 100), Sales(2017, 15), Sales(2018, 900),\n",
    "             Sales(2019, 50), Sales(2020, 750), Sales(2021, 950), Sales(2022, 400)]\n",
    "\n",
    "salesDS: DataFrame = spark.createDataFrame(salesList)\n",
    "\n",
    "sales2017Row: Row = salesDS.filter(salesDS[\"year\"] == 2017).first()\n",
    "sales2017: Sales = Sales(sales2017Row[\"year\"], sales2017Row[\"euros\"])\n",
    "print(f\"Sales for 2017 is {sales2017.euros}\")\n",
    "\n",
    "maximumSalesRow: Row = salesDS.orderBy(\"euros\", ascending=False).first()\n",
    "maximumSales: Sales = Sales(maximumSalesRow[\"year\"], maximumSalesRow[\"euros\"])\n",
    "print(f\"Maximum sales: year = {maximumSales.year}, euros = {maximumSales.euros}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8bc215-51f0-4c68-9bbd-0e36721599a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Example output for task 4:\n",
    "```text\n",
    "Sales for 2017 is 15\n",
    "Maximum sales: year = 2021, euros = 950\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc91c6c6-d116-43da-9c83-68490c00e5d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 5 - Dataset 2\n",
    "\n",
    "Continuation from task 4.\n",
    "The new sales list `multiSalesList` contains sales information from multiple sources and thus can contain multiple values for each year. The total sales in euros for a year is the sum of all the individual values for that year.\n",
    "\n",
    "Query for the sales on 2016 and the year with the highest amount of sales in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab8f3e0-b19b-462c-90ef-17cc44853d18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sales for 2016 is 705\nMaximum total sales: year = 2018, euros = 1027\n"
     ]
    }
   ],
   "source": [
    "multiSalesList: List[Sales] =  [\n",
    "    Sales(2015, 325), Sales(2016, 100), Sales(2017, 15), Sales(2018, 900),\n",
    "    Sales(2019, 50), Sales(2020, 750), Sales(2021, 950), Sales(2022, 400),\n",
    "    Sales(2016, 250), Sales(2017, 600), Sales(2019, 75), Sales(2016, 5),\n",
    "    Sales(2018, 127), Sales(2019, 200), Sales(2020, 225), Sales(2016, 350)\n",
    "]\n",
    "\n",
    "multiSalesDS: DataFrame = spark.createDataFrame([Row(year=sale.year, euros=sale.euros) for sale in multiSalesList])\n",
    "\n",
    "multiSales2016Row = multiSalesDS.filter(multiSalesDS[\"year\"] == 2016).agg(sum(\"euros\").alias(\"total_sales\")).first()\n",
    "multiSales2016: Sales = Sales(2016, multiSales2016Row[\"total_sales\"])\n",
    "print(f\"Total sales for 2016 is {multiSales2016.euros}\")\n",
    "\n",
    "maximumMultiSalesRow = multiSalesDS.groupBy(\"year\").agg(sum(\"euros\").alias(\"total_sales\")).orderBy(\"total_sales\", ascending=False).first()\n",
    "maximumMultiSales: Sales = Sales(maximumMultiSalesRow[\"year\"], maximumMultiSalesRow[\"total_sales\"])\n",
    "print(f\"Maximum total sales: year = {maximumMultiSales.year}, euros = {maximumMultiSales.euros}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4318b4-d3b1-4dfd-bf1e-52bb0eb62fbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output for task 5:\n",
    "\n",
    "```text\n",
    "...\n",
    "Total sales for 2016 is 705\n",
    "Maximum total sales: year = 2018, euros = 1027\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5949d41d-3e84-4eb1-a801-4e78e93ce5e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 6 - Streaming data from retail data\n",
    "\n",
    "Create a streaming data frame for similar retailer data as was used in tasks 1-3.\n",
    "\n",
    "In this exercise, streaming data is simulated by copying CSV files in 10 second intervals from a source folder to a target folder. The script for doing the file copying is given in task 8 and should not be run before the tasks 6 and 7 have been done.\n",
    "\n",
    "The target folder will be defined to be in the students storage as `ex3/YOUR_EX3_FOLDER` with the value for YOUR_EX3_FOLDER chosen by you.\n",
    "\n",
    "Hint: Spark cannot infer the schema of streaming data, so you have to give it explicitly. You can assume that the streaming data will have the same format as the static data used in tasks 1-3.\n",
    "\n",
    "Finally, note that you cannot really test this task before you have also done the tasks 7 and 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e14c8513-9d95-4fae-92c7-b215f594d171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Put your own unique folder name to the variable (use only letters, numbers, and underscores):\n",
    "#my_ex3_folder: str = ???\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"RetailerStreaming\").getOrCreate()\n",
    "\n",
    "# Replace YOUR_EX3_FOLDER with your unique folder name\n",
    "my_ex3_folder: str = \"ANISULMAHMUD_ex3\"\n",
    "\n",
    "# Define the target folder path\n",
    "targetFiles: str = f\"abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/{my_ex3_folder}/*\"\n",
    "\n",
    "# Define the schema for the streaming data (assuming it's the same as the static data)\n",
    "schema = \"ORDERNUMBER INT, QUANTITYORDERED INT, PRICEEACH DOUBLE, ORDERLINENUMBER INT, \" \\\n",
    "         \"SALES DOUBLE, ORDERDATE DATE, STATUS STRING, QTR_ID INT, MONTH_ID INT, \" \\\n",
    "         \"YEAR_ID INT, PRODUCTLINE STRING, MSRP DOUBLE, PRODUCTCODE STRING, CUSTOMERNAME STRING, \" \\\n",
    "         \"PHONE STRING, ADDRESSLINE1 STRING, ADDRESSLINE2 STRING, CITY STRING, STATE STRING, \" \\\n",
    "         \"POSTALCODE STRING, COUNTRY STRING, TERRITORY STRING, CONTACTLASTNAME STRING, CONTACTFIRSTNAME STRING, \" \\\n",
    "         \"DEALSIZE STRING\"\n",
    "\n",
    "# Create the streaming DataFrame\n",
    "retailerStreamingDF = (\n",
    "    spark.readStream.format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(targetFiles)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0800d727-7847-45fb-bb28-fa31913e87b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that you cannot really test this task before you have also done the tasks 7 and 8, i.e. there is no checkable output from this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e392d5c-a7bc-4d59-b841-f956acb86124",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 7 - The best selling days for the streaming data\n",
    "\n",
    "Find the best selling days using the streaming data frame from task 6.\n",
    "\n",
    "Note that in this task with the streaming data you don't need to limit the content only to the best 12 selling days like was done in task 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1766a936-4797-4dfb-a27a-a28069174614",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ORDERDATE: date (nullable = true)\n |-- total_sales: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BestSellingDaysStreaming\").getOrCreate()\n",
    "\n",
    "#total sales for each day in the streaming data\n",
    "bestDaysDFStreaming = (\n",
    "    retailerStreamingDF.withColumn(\"total_sales\", col(\"QUANTITYORDERED\") * col(\"PRICEEACH\"))\n",
    "    .groupBy(\"ORDERDATE\")\n",
    "    .agg({\"total_sales\": \"sum\"})\n",
    "    .select(\"ORDERDATE\", \"sum(total_sales)\")\n",
    "    .withColumnRenamed(\"sum(total_sales)\", \"total_sales\")\n",
    ")\n",
    "\n",
    "# Print the schema of the streaming DataFrame\n",
    "bestDaysDFStreaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00eb7ce9-d53b-4e35-bbeb-b257e55057cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that you cannot really test this task before you have also done the tasks 6 and 8, i.e. there is no checkable output from this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cde863-3795-406d-92a4-510f8de4acd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 8 - Test your streaming data solution\n",
    "\n",
    "Test your streaming data solution by following the output from the display command in the next cell while the provided test script in the following cell is running.\n",
    "\n",
    "- The test script copies files one by one (in ten second intervals) from the [shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) folder `exercises/ex3` to the [student container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2023-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2023gen2/path/students/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) folder `/ex3/MY_EX3_FOLDER` where `MY_EX3_FOLDER` is the folder name you chose in task 6.\n",
    "- To properly run the streaming data test, the target folder should be either empty or it should not exist at all. If there are files in the target folder, those are read immediately and the streaming data demostration will not work as intended. The script does try to remove all copied files from the target folder at the end, but that only happens if the script is not interrupted.\n",
    "\n",
    "To gain points from this task, answer the questions in the final cell of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1612b346-f5d3-4879-9e17-dd743883de3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ORDERDATE</th><th>total_sales</th></tr></thead><tbody><tr><td>null</td><td>8290886.790000001</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         8290886.790000001
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ORDERDATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "total_sales",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in Databricks the display function can be used to display also a streaming data frame\n",
    "# when developing outside this kind of environment, you need to create a query that could then be used to monitor the state of the data frame\n",
    "# Usually, when using streaming data the results are directed to some data storage, not just displayed like in this exercise.\n",
    "\n",
    "# There should be no need to edit anything in this cell!\n",
    "print(f\"Starting stream myQuery_{my_ex3_folder}\")\n",
    "display(bestDaysDFStreaming.limit(6), streamName=f\"myQuery_{my_ex3_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a672ff0-9e26-4551-8fd0-3e1f57c24379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file xaa.csv (1/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xaa.csv\nCopied file xab.csv (2/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xab.csv\nCopied file xac.csv (3/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xac.csv\nCopied file xad.csv (4/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xad.csv\nCopied file xae.csv (5/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xae.csv\nCopied file xaf.csv (6/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xaf.csv\nCopied file xag.csv (7/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xag.csv\nCopied file xah.csv (8/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xah.csv\nCopied file xai.csv (9/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xai.csv\nCopied file xaj.csv (10/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3/xaj.csv\nStopping stream myQuery_ANISULMAHMUD_ex3\nRemoved all copied files from abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/ANISULMAHMUD_ex3\n"
     ]
    }
   ],
   "source": [
    "# There should be no need to edit anything in this cell, but you can try to adjust to time variables.\n",
    "\n",
    "import glob\n",
    "import pathlib\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "initial_wait_time: float = 20.0\n",
    "time_interval: float = 10.0\n",
    "post_loop_wait_time: float = 20.0\n",
    "\n",
    "time.sleep(initial_wait_time)\n",
    "input_file_list: list = dbutils.fs.ls(\"abfss://shared@tunics320f2023gen2.dfs.core.windows.net/exercises/ex3/streamingData\")\n",
    "for index, csv_file in enumerate(input_file_list, start=1):\n",
    "    input_file_path = csv_file.path\n",
    "    input_file = pathlib.Path(input_file_path).name\n",
    "    output_file_path = f\"abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/{my_ex3_folder}/{input_file}\"\n",
    "    dbutils.fs.cp(input_file_path, output_file_path)\n",
    "    print(f\"Copied file {input_file} ({index}/{len(input_file_list)}) to {output_file_path}\")\n",
    "    time.sleep(time_interval)\n",
    "time.sleep(post_loop_wait_time)\n",
    "\n",
    "# stop updating the display for the streaming data frame\n",
    "for stream in spark.streams.active:\n",
    "    if stream.name == f\"myQuery_{my_ex3_folder}\":\n",
    "        print(f\"Stopping stream {stream.name}\")\n",
    "        spark.streams.get(stream.id).stop()\n",
    "\n",
    "# remove the copied files from the output folder\n",
    "for copy_file in dbutils.fs.ls(f\"abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/{my_ex3_folder}\"):\n",
    "    dbutils.fs.rm(copy_file.path)\n",
    "print(f\"Removed all copied files from abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/{my_ex3_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574795e7-d2fd-4dd7-8350-ded930f8db7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example output from the test script in task 8:\n",
    "\n",
    "```text\n",
    "Copied file xaa.csv (1/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xaa.csv\n",
    "Copied file xab.csv (2/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xab.csv\n",
    "Copied file xac.csv (3/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xac.csv\n",
    "Copied file xad.csv (4/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xad.csv\n",
    "Copied file xae.csv (5/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xae.csv\n",
    "Copied file xaf.csv (6/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xaf.csv\n",
    "Copied file xag.csv (7/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xag.csv\n",
    "Copied file xah.csv (8/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xah.csv\n",
    "Copied file xai.csv (9/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xai.csv\n",
    "Copied file xaj.csv (10/10) to abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here/xaj.csv\n",
    "Stopping stream myQuery_some_folder_here\n",
    "Removed all copied files from abfss://students@tunics320f2023gen2.dfs.core.windows.net/ex3/some_folder_here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057b6ebd-0b52-47de-982d-b062f8cb4d39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Answer the questions to get the points from task 8.**\n",
    "\n",
    "###How well did the streaming data example work for you?\n",
    "--  To give this answer, I can say that till now sometime the example is working for me and sometime I just face some problem. But in the end I came up with solution to my problem. But the example is working I can say that. \n",
    "\n",
    "###What was the final output for the streaming data for you?\n",
    "-- The output is  |-- ORDERDATE: date (nullable = true)\n",
    " |-- total_sales: double (nullable = true)\n",
    "\n",
    "###The data in the streaming tasks is the same as the earlier static data (just divided into multiple files).\n",
    "Did the final output match the first six rows of the task 2 output? \n",
    "-- No, the final output does not match the first six rows of the task 2 output.\n",
    "\n",
    "###If it did not, what do you think could be the reason?\n",
    "-- My thinking regarding this matter is that, I used static data for Task 2, where all of the data was available at once. I worked with streaming data in Task 7, and it's possible that time windows or micro-batches are being used for the computation. Based on the data available at the beginning of each window, the results would be established. So this can be the reason for the output mismatch. \n",
    "\n",
    "###If you had problems, what were they?\n",
    "###And what do you think were the causes for those problems?\n",
    "-- The problems are not that kinds of major but I must say. I have to follow the course slides, lectures to complete these task. Maybe if I not follow these regularly than maybe I can face problems. \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Week3_Md Anisul Islam Mahmud",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
